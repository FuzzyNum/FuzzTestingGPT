{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.dynamic_module_utils import get_imports\n",
    "from datasets import load_dataset\n",
    "from unittest.mock import patch\n",
    "import os\n",
    "\n",
    "\n",
    "def fixed_get_imports(filename: str | os.PathLike) -> list[str]:\n",
    "    imports = get_imports(filename)\n",
    "    if not torch.cuda.is_available() and \"flash_attn\" in imports:\n",
    "        imports.remove(\"flash_attn\")\n",
    "    return imports\n",
    "\n",
    "class DeepSeekMMLUEvaluation:\n",
    "    def __init__(self, model_name=\"deepseek-ai/DeepSeek-R1\", dataset_name=\"tasksource/mmlu\"):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model_name = 'qnguyen3/nanoLLaVA-1.5'\n",
    "        with patch(\"transformers.dynamic_module_utils.get_imports\", fixed_get_imports):\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                    device_map='auto' if torch.cuda.is_available() else None,\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                        model_name,\n",
    "                        trust_remote_code=True\n",
    "                    )\n",
    "        dataset = load_dataset(\"tasksource/mmlu\", \"anatomy\")\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "        return inputs\n",
    "    \n",
    "    def evaluate(self, split=\"validation\"):\n",
    "        dataset_split = self.dataset[split]\n",
    "        correct = 0\n",
    "        total = len(dataset_split)\n",
    "        \n",
    "        for item in dataset_split:\n",
    "            question = item[\"question\"]\n",
    "            choices = item[\"choices\"]\n",
    "            correct_answer = item[\"answer\"]\n",
    "            \n",
    "            inputs = self.preprocess(question + \"\\n\" + \"\\n\".join(choices))\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            predicted_answer = torch.argmax(logits).item()\n",
    "            \n",
    "            if predicted_answer == correct_answer:\n",
    "                correct += 1\n",
    "        \n",
    "        accuracy = correct / total * 100\n",
    "        return accuracy\n",
    "    \n",
    "    def inject_random_bit_flip(self):\n",
    "        import numpy as np\n",
    "        param_name, param_tensor = random.choice(list(self.model.named_parameters()))\n",
    "        param_data = param_tensor.detach().cpu().numpy()\n",
    "        param_flat = param_data.flatten()\n",
    "        byte_index = random.randint(0, param_flat.nbytes - 1) \n",
    "        bit_position = random.randint(0, 7)  \n",
    "        param_as_bytes = np.frombuffer(param_flat.view(np.uint8), dtype=np.uint8)\n",
    "        param_as_bytes[byte_index] ^= 1 << bit_position\n",
    "        modified_param = torch.tensor(\n",
    "            param_flat.reshape(param_data.shape), dtype=param_tensor.dtype\n",
    "        )\n",
    "        param_tensor.data = modified_param.to(param_tensor.device)\n",
    "\n",
    "        return param_name, byte_index, bit_position\n",
    "    \n",
    "    def run_experiment(self, num_flips=5):\n",
    "        original_accuracy = self.evaluate()\n",
    "        print(f\"Original MMLU Accuracy: {original_accuracy:.2f}%\")\n",
    "        \n",
    "        flip_results = []\n",
    "        for i in range(num_flips):\n",
    "            param_name, byte_index, bit_position = self.inject_random_bit_flip()\n",
    "            new_accuracy = self.evaluate()\n",
    "            flip_results.append((param_name, byte_index, bit_position, new_accuracy))\n",
    "            print(f\"Flip {i+1}: {param_name}, Byte {byte_index}, Bit {bit_position}, Accuracy: {new_accuracy:.2f}%\")\n",
    "        \n",
    "        return flip_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original MMLU Accuracy: 0.00%\n",
      "Flip 1: model.layers.7.self_attn.q_proj.weight, Byte 1610207, Bit 4, Accuracy: 0.00%\n",
      "Flip 2: model.layers.19.mlp.down_proj.weight, Byte 9689436, Bit 4, Accuracy: 0.00%\n",
      "Flip 3: model.layers.1.self_attn.o_proj.weight, Byte 3591366, Bit 6, Accuracy: 0.00%\n",
      "Flip 4: model.layers.18.self_attn.k_proj.weight, Byte 3943560, Bit 5, Accuracy: 0.00%\n",
      "Flip 5: model.layers.15.self_attn.v_proj.weight, Byte 3364119, Bit 5, Accuracy: 0.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('model.layers.7.self_attn.q_proj.weight', 1610207, 4, 0.0),\n",
       " ('model.layers.19.mlp.down_proj.weight', 9689436, 4, 0.0),\n",
       " ('model.layers.1.self_attn.o_proj.weight', 3591366, 6, 0.0),\n",
       " ('model.layers.18.self_attn.k_proj.weight', 3943560, 5, 0.0),\n",
       " ('model.layers.15.self_attn.v_proj.weight', 3364119, 5, 0.0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepseek=DeepSeekMMLUEvaluation()\n",
    "deepseek.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfd2abd5366461ab50c2191b3ba0c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b0350745b94d8083a93cba7c522269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mmlu.py:   0%|          | 0.00/5.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7346e01698344103940b263b67838add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/19.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd4ae37f67e4db3920449e56f432a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/4.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13c2f3704ab4834ac42a7053c538aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/3.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040d7bce5dc54026bddb50f7a6e13e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82b0b03c69d4c629ce08874f6bb7ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea210619e5241e4851cf99cc7dd0070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"tasksource/mmlu\", \"anatomy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DeepSeekMMLUEvaluation:\n",
    "    def __init__(self, model_name=\"deepseek-ai/DeepSeek-R1\", device=None):\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1\", trust_remote_code=True)\n",
    "        self.original_state_dict = {k: v.clone() for k, v in self.model.state_dict().items()}\n",
    "    \n",
    "    def format_mmlu_question(self, question, choices):\n",
    "        \"\"\"Formats the MMLU question for causal LM input.\"\"\"\n",
    "        formatted = f\"Question: {question}\\n\"\n",
    "        for i, choice in enumerate(choices):\n",
    "            formatted += f\"{chr(65+i)}) {choice}\\n\"\n",
    "        formatted += \"Answer: \"\n",
    "        return formatted\n",
    "    \n",
    "    def evaluate_choice_log_likelihood(self, question, choices):\n",
    "        \"\"\"Evaluates the log-likelihood for each multiple-choice answer.\"\"\"\n",
    "        prompt = self.format_mmlu_question(question, choices)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        choice_probs = []\n",
    "        \n",
    "        for choice in choices:\n",
    "            choice_input = self.tokenizer(choice, return_tensors=\"pt\").to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs, labels=inputs.input_ids)\n",
    "                log_probs = outputs.loss.item()\n",
    "                choice_probs.append(-log_probs)  # Negate loss to get likelihood\n",
    "        \n",
    "        return choices[np.argmax(choice_probs)]  # Return most likely choice\n",
    "    \n",
    "    def evaluate(self, dataset):\n",
    "        \"\"\"Evaluates model accuracy on MMLU dataset.\"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for question, choices, answer in dataset:\n",
    "            predicted_answer = self.evaluate_choice_log_likelihood(question, choices)\n",
    "            if predicted_answer == answer:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        \n",
    "        return (correct / total) * 100\n",
    "    \n",
    "    def inject_random_bit_flip(self):\n",
    "        \"\"\"Injects a random bit flip into the model weights.\"\"\"\n",
    "        param_name, param_tensor = random.choice(list(self.model.named_parameters()))\n",
    "        param_data = param_tensor.detach().cpu().numpy()\n",
    "        param_flat = param_data.flatten()\n",
    "        byte_index = random.randint(0, param_flat.nbytes - 1)\n",
    "        bit_position = random.randint(0, 7)\n",
    "        \n",
    "        param_as_bytes = np.frombuffer(param_flat.view(np.uint8), dtype=np.uint8)\n",
    "        param_as_bytes[byte_index] ^= 1 << bit_position\n",
    "        \n",
    "        modified_param = torch.tensor(param_flat.reshape(param_data.shape), dtype=param_tensor.dtype).to(self.device)\n",
    "        param_tensor.data = modified_param\n",
    "        \n",
    "        return param_name, byte_index, bit_position\n",
    "    \n",
    "    def reset_model_weights(self):\n",
    "        \"\"\"Resets model weights to their original state.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.model.named_parameters():\n",
    "                param.copy_(self.original_state_dict[name])\n",
    "    \n",
    "    def run_experiment(self, dataset, num_flips=10):\n",
    "        \"\"\"Runs the accuracy evaluation with and without bit flips.\"\"\"\n",
    "        original_accuracy = self.evaluate(dataset)\n",
    "        print(f\"Original Accuracy: {original_accuracy:.2f}%\")\n",
    "        \n",
    "        flip_results = []\n",
    "        for i in range(num_flips):\n",
    "            param_name, byte_index, bit_position = self.inject_random_bit_flip()\n",
    "            new_accuracy = self.evaluate(dataset)\n",
    "            flip_results.append((param_name, byte_index, bit_position, new_accuracy))\n",
    "            print(f\"Flip {i+1}: {param_name}, Byte {byte_index}, Bit {bit_position}, Accuracy: {new_accuracy:.2f}%\")\n",
    "            self.reset_model_weights()\n",
    "        \n",
    "        return flip_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown quantization type, got fp8 - supported types are: ['awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'eetq', 'higgs', 'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao', 'bitnet', 'vptq']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m deepseek_eval \u001b[38;5;241m=\u001b[39m \u001b[43mDeepSeekMMLUEvaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load MMLU dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m deepseek_eval\u001b[38;5;241m.\u001b[39mload_mmlu(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhigh_school_math\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m, in \u001b[0;36mDeepSeekMMLUEvaluation.__init__\u001b[0;34m(self, model_name, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeepseek-ai/DeepSeek-R1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_state_dict \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict()\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:559\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    558\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/modeling_utils.py:3605\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_quantized \u001b[38;5;129;01mor\u001b[39;00m quantization_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pre_quantized:\n\u001b[0;32m-> 3605\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoHfQuantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_quantization_configs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3606\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\n\u001b[1;32m   3607\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3608\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3609\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m quantization_config\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/quantizers/auto.py:181\u001b[0m, in \u001b[0;36mAutoHfQuantizer.merge_quantization_configs\u001b[0;34m(cls, quantization_config, quantization_config_from_args)\u001b[0m\n\u001b[1;32m    178\u001b[0m     warning_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(quantization_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 181\u001b[0m     quantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoQuantizationConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(quantization_config, (GPTQConfig, AwqConfig, FbgemmFp8Config, CompressedTensorsConfig))\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m quantization_config_from_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m ):\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# special case for GPTQ / AWQ / FbgemmFp8 config collision\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     loading_attr_dict \u001b[38;5;241m=\u001b[39m quantization_config_from_args\u001b[38;5;241m.\u001b[39mget_loading_attributes()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/quantizers/auto.py:105\u001b[0m, in \u001b[0;36mAutoQuantizationConfig.from_dict\u001b[0;34m(cls, quantization_config_dict)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms quantization config from the arguments has no `quant_method` attribute. Make sure that the model has been correctly quantized\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m     )\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m quant_method \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m AUTO_QUANTIZATION_CONFIG_MAPPING\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown quantization type, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquant_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - supported types are:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(AUTO_QUANTIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m     )\n\u001b[1;32m    110\u001b[0m target_cls \u001b[38;5;241m=\u001b[39m AUTO_QUANTIZATION_CONFIG_MAPPING[quant_method]\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target_cls\u001b[38;5;241m.\u001b[39mfrom_dict(quantization_config_dict)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown quantization type, got fp8 - supported types are: ['awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'eetq', 'higgs', 'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao', 'bitnet', 'vptq']"
     ]
    }
   ],
   "source": [
    "deepseek_eval = DeepSeekMMLUEvaluation()\n",
    "\n",
    "# Load MMLU dataset\n",
    "train_dataset = deepseek_eval.load_mmlu(subset=\"high_school_math\", split=\"train\")\n",
    "val_dataset = deepseek_eval.load_mmlu(subset=\"high_school_math\", split=\"test\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Train the model\n",
    "deepseek_eval.train(train_dataloader)\n",
    "\n",
    "# Run bit-flip experiment\n",
    "deepseek_eval.run_experiment(val_dataloader, num_flips=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
