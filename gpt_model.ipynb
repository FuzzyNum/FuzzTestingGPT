{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"metadata":{}},"outputs":[],"source":["import sys\n","sys.path.append(\"../../Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages\")"]},{"cell_type":"code","execution_count":2,"metadata":{"metadata":{}},"outputs":[],"source":["from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n","from datasets import load_dataset\n","import torch\n","import textwrap\n","import warnings\n","warnings.filterwarnings('ignore')\n","import random"]},{"cell_type":"code","execution_count":3,"metadata":{"metadata":{}},"outputs":[{"name":"stdout","output_type":"stream","text":["Using model:  gpt2-large\n"]}],"source":["model_to_use = \"gpt2-large\"\n","print(\"Using model: \", model_to_use)\n","tokenizer = GPT2TokenizerFast.from_pretrained(model_to_use)\n","model = GPT2LMHeadModel.from_pretrained(model_to_use,output_scores=True,pad_token_id=tokenizer.eos_token_id)"]},{"cell_type":"code","execution_count":4,"metadata":{"metadata":{}},"outputs":[{"data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 1280)\n","    (wpe): Embedding(1024, 1280)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-35): 36 x GPT2Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",")"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["model.eval()"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import random\n","from transformers import GPT2Model, GPT2Tokenizer\n","import matplotlib.pyplot as plt\n","\n","layers_to_track = ['transformer.h.0.attn.c_attn', 'transformer.h.0.attn.c_proj', \n","                   'transformer.h.1.attn.c_attn', 'transformer.h.1.attn.c_proj',\n","                   'transformer.h.2.attn.c_attn', 'transformer.h.2.attn.c_proj', \n","                   'transformer.h.3.attn.c_attn', 'transformer.h.3.attn.c_proj']\n","\n","class NeuronCoverage:\n","    def __init__(self, model, tokenizer, top_k=100, activation_threshold=0.1):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.activations = {}\n","        self.coverage_log = []\n","        self.handles = []\n","        self.max_length = 300\n","        self.top_k = top_k  # For top-K coverage\n","        self.activation_threshold = activation_threshold  # Dynamic threshold\n","        self.register_hooks()\n","\n","    def register_hooks(self):\n","        for name, module in self.model.named_modules():\n","            if any(layer_name in name for layer_name in layers_to_track):\n","                handle = module.register_forward_hook(self.create_hook(name))\n","                self.handles.append(handle)\n","\n","    def create_hook(self, layer_name):\n","        def hook(module, input, output):\n","            activated_neurons = F.relu(output)\n","            self.activations[layer_name] = activated_neurons.detach().cpu().numpy()\n","        return hook\n","\n","    def compute_coverage(self):\n","        coverage = {}\n","        top_k_coverage = {}\n","\n","        for layer_name, activation in self.activations.items():\n","            # Threshold-based coverage\n","            above_threshold = activation > self.activation_threshold\n","            coverage[layer_name] = above_threshold.sum() / above_threshold.size\n","\n","            # Top-K coverage\n","            top_k_values = np.partition(activation.flatten(), -self.top_k)[-self.top_k:]\n","            # Count how many of these top-k values are above the threshold\n","            top_k_above_threshold = np.sum(top_k_values > self.activation_threshold)\n","            top_k_coverage[layer_name] = top_k_above_threshold / self.top_k if self.top_k > 0 else 0.0\n","\n","        # Log results\n","        self.coverage_log.append((coverage, top_k_coverage))\n","        # Reset activations for the next run\n","        self.activations = {}\n","\n","        return coverage, top_k_coverage\n","\n","    def remove_hooks(self):\n","        for handle in self.handles:\n","            handle.remove()\n","\n","    def process_input(self, text):\n","        inputs = self.tokenizer(text, return_tensors=\"pt\")\n","        return inputs\n","\n","    def get_model_output(self, inputs, max_length=None):\n","        max_length = max_length or self.max_length\n","        input_ids = inputs['input_ids']\n","\n","        with torch.no_grad():\n","            output_sequences = self.model.generate(\n","                input_ids=input_ids,\n","                max_length=max_length,\n","                num_return_sequences=1,\n","                no_repeat_ngram_size=2,\n","                pad_token_id=self.tokenizer.eos_token_id,\n","                temperature=1.0,\n","                top_k=50,\n","                top_p=0.95\n","            )\n","\n","        return output_sequences\n","\n","    def decode_output(self, output_sequences):\n","        generated_text = self.tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n","        return generated_text\n","\n","    def run_coverage_analysis(self, text, apply_mutations=False):\n","        if apply_mutations:\n","            text = self.apply_mutations(text)\n","\n","        inputs = self.process_input(text)\n","        output_sequences = self.get_model_output(inputs)\n","        coverage, top_k_coverage = self.compute_coverage()\n","        decoded_text = self.decode_output(output_sequences)\n","\n","        return coverage, top_k_coverage, decoded_text, text\n","\n","    def run_multiple_experiments(self, text, num_runs=5, apply_mutations=False):\n","        for _ in range(num_runs):\n","            self.run_coverage_analysis(text, apply_mutations)\n","\n","    def visualize_coverage(self):\n","        # Extract and plot coverage history for each layer\n","        layers = list(self.coverage_log[0][0].keys())\n","        coverage_data = {layer: [entry[0][layer] for entry in self.coverage_log] for layer in layers}\n","        top_k_data = {layer: [entry[1][layer] for entry in self.coverage_log] for layer in layers}\n","\n","        plt.figure(figsize=(12, 8))\n","        for layer, data in coverage_data.items():\n","            plt.plot(data, label=f\"{layer} Coverage\")\n","        \n","        plt.xlabel(\"Iteration\")\n","        plt.ylabel(\"Coverage Percentage\")\n","        plt.title(\"Neuron Coverage over Time\")\n","        plt.legend()\n","        plt.show()\n","\n","        # Visualize top-K coverage\n","        plt.figure(figsize=(12, 8))\n","        for layer, data in top_k_data.items():\n","            plt.plot(data, label=f\"{layer} Top-K Coverage\")\n","        \n","        plt.xlabel(\"Iteration\")\n","        plt.ylabel(\"Top-K Coverage Percentage\")\n","        plt.title(\"Top-K Neuron Coverage over Time\")\n","        plt.legend()\n","        plt.show()\n","\n","    # Additional Mutation Functions (unchanged)\n","    def apply_mutations(self, text, num_choices=2):\n","        for _ in range(num_choices):\n","            mutation_type = random.choice([self.replace_words, self.insert_words, self.reorder_words,\n","                                           self.swap_characters, self.random_insertion, self.random_deletion])\n","            text = mutation_type(text)\n","        return text\n","\n","    def replace_words(self, text):\n","        words = text.split()\n","        if len(words) > 0:\n","            random_idx = random.randint(0, len(words) - 1)\n","            words[random_idx] = self.generate_random_word()\n","        return \" \".join(words)\n","\n","    def insert_words(self, text):\n","        words = text.split()\n","        if len(words) > 0:\n","            random_idx = random.randint(0, len(words) - 1)\n","            words.insert(random_idx, self.generate_random_word())\n","        return \" \".join(words)\n","\n","    def reorder_words(self, text):\n","        words = text.split()\n","        random.shuffle(words)\n","        return \" \".join(words)\n","\n","    def swap_characters(self, text):\n","        idx = random.randint(0, len(text) - 2)\n","        mutated_text = text[:idx] + text[idx+1] + text[idx] + text[idx+2:]\n","        return mutated_text\n","\n","    def random_insertion(self, text):\n","        idx = random.randint(0, len(text) - 1)\n","        char = random.choice('abcdefghijklmnopqrstuvwxyz')\n","        mutated_text = text[:idx] + char + text[idx:]\n","        return mutated_text\n","\n","    def random_deletion(self, text):\n","        idx = random.randint(0, len(text) - 1)\n","        mutated_text = text[:idx] + text[idx+1:]\n","        return mutated_text\n","\n","    def generate_random_word(self):\n","        return ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=5))\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["coverage_tracker = NeuronCoverage(model,tokenizer)\n","coverage_tracker.register_hooks()"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["input_text = \"When in the Course of human events, it becomes necessary for one people \"\n","\n","# Run coverage analysis multiple times\n","coverage_tracker.run_multiple_experiments(input_text, num_runs=10, apply_mutations=True)\n","\n","# Visualize neuron coverage history over multiple iterations\n","coverage_tracker.visualize_coverage()\n","\n","# Remove hooks to clean up\n","coverage_tracker.remove_hooks()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["coverage_tracker.remove_hooks()"]},{"cell_type":"code","execution_count":20,"metadata":{"metadata":{}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import GPT2Model, GPT2Tokenizer\n","\n","layers_to_track=['transformer.h.0.attn.c_attn','transformer.h.0.attn.c_proj','transformer.h.1.attn.c_attn','transformer.h.1.attn.c_proj','transformer.h.2.attn.c_attn','transformer.h.2.attn.c_proj','transformer.h.3.attn.c_attn','transformer.h.3.attn.c_proj']\n","\n","class NeuronCoverage:\n","    def __init__(self, model, tokenizer):\n","        self.model = model\n","        self.activations = {}\n","        self.tokenizer = tokenizer\n","        self.handles = []\n","        self.max_length=300\n","\n","    def register_hooks(self):\n","        for name, module in self.model.named_modules():\n","            if any(layer_name in name for layer_name in layers_to_track):\n","                handle = module.register_forward_hook(self.create_hook(name))\n","                self.handles.append(handle)\n","\n","    def create_hook(self, layer_name):\n","        def hook(module, input, output):\n","            # Apply ReLU activation\n","            activated_neurons = F.relu(output)\n","            # Count non-zero activations\n","            activated_neurons = activated_neurons > 0\n","            # Record activations per neuron\n","            self.activations[layer_name] = activated_neurons.detach().cpu().numpy()\n","        return hook\n","\n","    def compute_coverage(self):\n","        coverage = {}\n","        for layer_name, activation in self.activations.items():\n","            covered_neurons = activation.sum()\n","            total_neurons = activation.size\n","            coverage[layer_name] = covered_neurons / total_neurons\n","        return coverage\n","\n","    def remove_hooks(self):\n","        for handle in self.handles:\n","            handle.remove()\n","\n","    def process_input(self, text):\n","        \"\"\"Tokenizes input text and passes it through the model.\"\"\"\n","        inputs = self.tokenizer(text, return_tensors=\"pt\")\n","        return inputs\n","\n","    def get_model_output(self, inputs, max_length=None):\n","        max_length = max_length or self.max_length\n","        input_ids = inputs['input_ids']\n","\n","        with torch.no_grad():\n","            output_sequences = self.model.generate(\n","                input_ids=input_ids,\n","                max_length=max_length,\n","                num_return_sequences=1,\n","                no_repeat_ngram_size=2,\n","                pad_token_id=self.tokenizer.eos_token_id,\n","                temperature=1.0,\n","                top_k=50,\n","                top_p=0.95\n","            )\n","\n","        return output_sequences\n","\n","\n","\n","    def decode_output(self, output_sequences):\n","        generated_text = self.tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n","        return generated_text\n","\n","    \n","    def run_coverage_analysis(self, text, apply_mutations=False):\n","        if apply_mutations:\n","            text = self.apply_mutations(text)  # Apply mutations before processing\n","\n","        inputs = self.process_input(text)\n","        output_sequences = self.get_model_output(inputs)\n","        coverage = self.compute_coverage()\n","        decoded_text = self.decode_output(output_sequences)\n","\n","        return coverage, decoded_text, text\n","\n","    def apply_mutations(self, text, num_choices=2):\n","        for i in range(0,num_choices):\n","            mutation_type = random.choice([self.replace_words, self.insert_words, self.reorder_words,self.swap_characters,self.random_insertion,self.random_deletion])\n","            text = mutation_type(text)\n","        return text\n","\n","    def replace_words(self, text):\n","        words = text.split()\n","        if len(words) > 0:\n","            random_idx = random.randint(0, len(words) - 1)\n","            words[random_idx] = self.generate_random_word()  # Replace random word\n","        return \" \".join(words)\n","\n","    def insert_words(self, text):\n","        words = text.split()\n","        if len(words) > 0:\n","            random_idx = random.randint(0, len(words) - 1)\n","            words.insert(random_idx, self.generate_random_word())  # Insert random word\n","        return \" \".join(words)\n","\n","    def reorder_words(self, text):\n","        words = text.split()\n","        random.shuffle(words)  # Shuffle the words randomly\n","        return \" \".join(words)\n","    \n","    def swap_characters(self,text):\n","        idx = random.randint(0, len(text) - 2)\n","        mutated_text = text[:idx] + text[idx+1] + text[idx] + text[idx+2:]\n","        return mutated_text\n","\n","    def random_insertion(self,text):\n","        idx = random.randint(0, len(text) - 1)\n","        char = random.choice('abcdefghijklmnopqrstuvwxyz')\n","        mutated_text = text[:idx] + char + text[idx:]\n","        return mutated_text\n","\n","    def random_deletion(self,text):\n","        idx = random.randint(0, len(text) - 1)\n","        mutated_text = text[:idx] + text[idx+1:]\n","        return mutated_text\n","\n","    def generate_random_word(self):\n","        # For simplicity, generate a random word of length 5 from the alphabet\n","        return ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=5))\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":2}
