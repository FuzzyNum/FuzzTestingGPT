{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"metadata":{}},"outputs":[],"source":["import sys\n","sys.path.append(\"../../Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages\")"]},{"cell_type":"code","execution_count":21,"metadata":{"metadata":{}},"outputs":[],"source":["from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n","from datasets import load_dataset\n","import torch\n","import textwrap\n","import warnings\n","warnings.filterwarnings('ignore')\n","import random"]},{"cell_type":"code","execution_count":22,"metadata":{"metadata":{}},"outputs":[{"name":"stdout","output_type":"stream","text":["Using model:  gpt2-large\n"]}],"source":["model_to_use = \"gpt2-large\"\n","print(\"Using model: \", model_to_use)\n","tokenizer = GPT2TokenizerFast.from_pretrained(model_to_use)\n","model = GPT2LMHeadModel.from_pretrained(model_to_use,output_scores=True,pad_token_id=tokenizer.eos_token_id)"]},{"cell_type":"code","execution_count":23,"metadata":{"metadata":{}},"outputs":[{"data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 1280)\n","    (wpe): Embedding(1024, 1280)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-35): 36 x GPT2Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",")"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":32,"metadata":{"metadata":{}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import GPT2Model, GPT2Tokenizer\n","\n","layers_to_track=['transformer.h.0.attn.c_attn','transformer.h.0.attn.c_proj','transformer.h.1.attn.c_attn','transformer.h.1.attn.c_proj','transformer.h.2.attn.c_attn','transformer.h.2.attn.c_proj','transformer.h.3.attn.c_attn','transformer.h.3.attn.c_proj']\n","\n","class NeuronCoverage:\n","    def __init__(self, model, tokenizer):\n","        self.model = model\n","        self.activations = {}\n","        self.tokenizer = tokenizer\n","        self.handles = []\n","        self.max_length=100\n","\n","    def register_hooks(self):\n","        for name, module in self.model.named_modules():\n","            if any(layer_name in name for layer_name in layers_to_track):\n","                handle = module.register_forward_hook(self.create_hook(name))\n","                self.handles.append(handle)\n","\n","    def create_hook(self, layer_name):\n","        def hook(module, input, output):\n","            # Apply ReLU activation\n","            activated_neurons = F.relu(output)\n","            # Count non-zero activations\n","            activated_neurons = activated_neurons > 0\n","            # Record activations per neuron\n","            self.activations[layer_name] = activated_neurons.detach().cpu().numpy()\n","        return hook\n","\n","    def compute_coverage(self):\n","        coverage = {}\n","        for layer_name, activation in self.activations.items():\n","            covered_neurons = activation.sum()\n","            total_neurons = activation.size\n","            coverage[layer_name] = covered_neurons / total_neurons\n","        return coverage\n","\n","    def remove_hooks(self):\n","        for handle in self.handles:\n","            handle.remove()\n","\n","    def process_input(self, text):\n","        \"\"\"Tokenizes input text and passes it through the model.\"\"\"\n","        inputs = self.tokenizer(text, return_tensors=\"pt\")\n","        return inputs\n","\n","    def get_model_output(self, inputs, max_length=None):\n","        max_length = max_length or self.max_length\n","        input_ids = inputs['input_ids']\n","\n","        with torch.no_grad():\n","            output_sequences = self.model.generate(\n","                input_ids=input_ids,\n","                max_length=max_length,\n","                num_return_sequences=1,\n","                no_repeat_ngram_size=2,\n","                pad_token_id=self.tokenizer.eos_token_id,\n","                temperature=1.0,\n","                top_k=50,\n","                top_p=0.95\n","            )\n","\n","        return output_sequences\n","\n","\n","\n","    def decode_output(self, output_sequences):\n","        generated_text = self.tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n","        return generated_text\n","\n","    \n","    def run_coverage_analysis(self, text, apply_mutations=False):\n","        if apply_mutations:\n","            text = self.apply_mutations(text)  # Apply mutations before processing\n","\n","        inputs = self.process_input(text)\n","        output_sequences = self.get_model_output(inputs)\n","        coverage = self.compute_coverage()\n","        decoded_text = self.decode_output(output_sequences)\n","\n","        return coverage, decoded_text, text\n","\n","    def apply_mutations(self, text):\n","        mutation_type = random.choice([self.replace_words, self.insert_words, self.reorder_words])\n","        mutated_text = mutation_type(text)\n","        return mutated_text\n","\n","    def replace_words(self, text):\n","        words = text.split()\n","        if len(words) > 0:\n","            random_idx = random.randint(0, len(words) - 1)\n","            words[random_idx] = self.generate_random_word()  # Replace random word\n","        return \" \".join(words)\n","\n","    def insert_words(self, text):\n","        words = text.split()\n","        if len(words) > 0:\n","            random_idx = random.randint(0, len(words) - 1)\n","            words.insert(random_idx, self.generate_random_word())  # Insert random word\n","        return \" \".join(words)\n","\n","    def reorder_words(self, text):\n","        words = text.split()\n","        random.shuffle(words)  # Shuffle the words randomly\n","        return \" \".join(words)\n","\n","    def generate_random_word(self):\n","        # For simplicity, generate a random word of length 5 from the alphabet\n","        return ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=5))\n","\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["coverage_tracker = NeuronCoverage(model,tokenizer)\n","coverage_tracker.register_hooks()"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Layer transformer.h.0.attn.c_attn coverage: 49.19%\n","Layer transformer.h.0.attn.c_proj coverage: 50.94%\n","Layer transformer.h.1.attn.c_attn coverage: 48.49%\n","Layer transformer.h.1.attn.c_proj coverage: 50.78%\n","Layer transformer.h.2.attn.c_attn coverage: 49.43%\n","Layer transformer.h.2.attn.c_proj coverage: 47.58%\n","Layer transformer.h.3.attn.c_attn coverage: 51.77%\n","Layer transformer.h.3.attn.c_proj coverage: 52.42%\n","\n","Mutated Input: them When have it for connected of Course political necessary the which the one to people becomes dissolve human with bands in another, events,\n","\n","Generated Text Output:\n","them When have it for connected of Course political necessary the which the one to people becomes dissolve human with bands in another, events, and the other to the people, the same, or the opposite, to be the cause of the former, which is the case with the present, is not the reason why the first to them is dissolved, but the fact that the latter is in the power of those who are in power, that is, of their own will, not of that of others.\n","\n"]}],"source":["\n","# Compute neuron coverage\n","input_text = \"When in the Course of human events, it becomes necessary for one people to dissolve the political bands which have connected them with another, \"\n","\n","# Run with mutations applied\n","coverage, output_text, text = coverage_tracker.run_coverage_analysis(input_text, apply_mutations=True)\n","\n","for layer, cov in coverage.items():\n","    print(f\"Layer {layer} coverage: {cov:.2%}\")\n","\n","print(f\"\\nMutated Input: {text}\")\n","print(\"\\nGenerated Text Output:\")\n","print(output_text)\n","\n","\n","coverage_tracker.remove_hooks()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["coverage_tracker.remove_hooks()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":2}
