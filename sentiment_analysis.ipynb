{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "class SentimentAnalysisModel:\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", num_labels=2):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        return inputs\n",
    "\n",
    "    def predict(self, text):\n",
    "        inputs = self.preprocess(text)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1)\n",
    "        return predicted_class.item()\n",
    "\n",
    "    def train(self, train_dataloader, epochs=5, learning_rate=2e-5):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        self.model.train()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in train_dataloader:\n",
    "                input_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss, logits = outputs\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "    def evaluate(self, val_dataloader):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                _, predicted = torch.max(outputs.logits, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SentimentAnalysisModel:\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", num_labels=2):\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        self.model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        return inputs\n",
    "\n",
    "    def predict(self, text):\n",
    "        inputs = self.preprocess(text)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1)\n",
    "        return predicted_class.item()\n",
    "\n",
    "    def train(self, train_dataloader, epochs=5, learning_rate=2e-5):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        self.model.train()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in train_dataloader:\n",
    "                input_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss, logits = outputs\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "    def evaluate(self, val_dataloader):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                _, predicted = torch.max(outputs.logits, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        return accuracy\n",
    "    \n",
    "    def visualize_data(self, data, labels):\n",
    "        # Assuming data is a list of text strings and labels is a list of 0s and 1s\n",
    "        positive_examples = [text for text, label in zip(data, labels) if label == 1]\n",
    "        negative_examples = [text for text, label in zip(data, labels) if label == 0]\n",
    "\n",
    "        # Visualize a random sample of positive and negative examples\n",
    "        num_examples = 5\n",
    "        fig, axs = plt.subplots(2, num_examples, figsize=(15, 5))\n",
    "        for i in range(num_examples):\n",
    "            axs[0, i].text(0.5, 0.5, positive_examples[i], ha='center', va='center')\n",
    "            axs[0, i].axis('off')\n",
    "            axs[1, i].text(0.5, 0.5, negative_examples[i], ha='center', va='center')\n",
    "            axs[1, i].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "def inject_random_bit_flip(model):\n",
    "    # Get a random layer and parameter index\n",
    "    layer_index = random.randint(0, len(list(model.parameters())) - 1)\n",
    "    parameter_index = random.randint(0, len(model.parameters()[layer_index]) - 1)\n",
    "\n",
    "    # Get the parameter tensor\n",
    "    param = model.parameters()[layer_index][parameter_index]\n",
    "\n",
    "    # Get a random bit position\n",
    "    bit_position = random.randint(0, param.numel() * 8 - 1)\n",
    "\n",
    "    # Flip the bit at the specified position\n",
    "    param_data = param.data.numpy()\n",
    "    byte_index = bit_position // 8\n",
    "    bit_mask = 1 << (bit_position % 8)\n",
    "    param_data[byte_index] ^= bit_mask\n",
    "    param.data = torch.tensor(param_data, dtype=torch.float32)\n",
    "\n",
    "    return layer_index, parameter_index, bit_position\n",
    "\n",
    "def run_experiment(model, val_dataloader, num_flips):\n",
    "    original_accuracy = model.evaluate(val_dataloader)\n",
    "    print(f\"Original accuracy: {original_accuracy:.2f}%\")\n",
    "\n",
    "    flip_results = []\n",
    "    for i in range(num_flips):\n",
    "        layer_index, parameter_index, bit_position = inject_random_bit_flip(model)\n",
    "        new_accuracy = model.evaluate(val_dataloader)\n",
    "        flip_results.append((layer_index, parameter_index, bit_position, new_accuracy))\n",
    "        print(f\"Flip {i+1}: Layer {layer_index}, Parameter {parameter_index}, Bit {bit_position}, New accuracy: {new_accuracy:.2f}%\")\n",
    "\n",
    "    return flip_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Tokenize the text using the loaded tokenizer and convert it to PyTorch tensors\n",
    "        encoded_text = self.tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "        return encoded_text, label\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=.2)\n",
    "\n",
    "# Create the datasets and dataloaders\n",
    "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = SentimentDataset(val_texts, val_labels, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
