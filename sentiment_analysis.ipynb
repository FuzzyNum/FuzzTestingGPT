{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "class SentimentAnalysisModel:\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", num_labels=2):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        return inputs\n",
    "\n",
    "    def predict(self, text):\n",
    "        inputs = self.preprocess(text)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1)\n",
    "        return predicted_class.item()\n",
    "\n",
    "    def train(self, train_dataloader, epochs=5, learning_rate=2e-5):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        self.model.train()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in train_dataloader:\n",
    "                input_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss, logits = outputs\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "    def evaluate(self, val_dataloader):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                _, predicted = torch.max(outputs.logits, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "class TransformerEvaluator:\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", num_labels=2):\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        self.model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.original_weights = {name: param.clone() for name, param in self.model.named_parameters()}\n",
    "        self.bit_flip_log = []\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        return self.tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    def predict(self, text):\n",
    "        inputs = self.preprocess(text)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        return torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "                _, predicted = torch.max(outputs.logits, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        return accuracy\n",
    "\n",
    "    def inject_random_bit_flip(self):\n",
    "        param_list = list(self.model.named_parameters())\n",
    "        layer_index = random.randint(0, len(param_list) - 1)\n",
    "        layer_name, param = param_list[layer_index]\n",
    "\n",
    "        # Flatten the tensor and randomly select a bit\n",
    "        flat_param = param.data.view(-1)\n",
    "        param_index = random.randint(0, flat_param.numel() - 1)\n",
    "        original_value = flat_param[param_index].item()\n",
    "\n",
    "        # Convert to bitwise representation and flip a random bit\n",
    "        param_value = flat_param[param_index].item()\n",
    "        int_repr = np.float32(param_value).view(np.int32)\n",
    "        bit_position = random.randint(0, 31)\n",
    "        flipped_int = int_repr ^ (1 << bit_position)\n",
    "        flipped_value = np.float32(flipped_int).view(np.float32)\n",
    "\n",
    "        # Apply the flipped value\n",
    "        flat_param[param_index] = flipped_value\n",
    "        self.bit_flip_log.append((layer_name, param_index, bit_position, original_value, flipped_value))\n",
    "\n",
    "        return layer_name, param_index, bit_position, original_value, flipped_value\n",
    "\n",
    "    def reset_weights(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            param.data.copy_(self.original_weights[name])\n",
    "        self.bit_flip_log.clear()\n",
    "\n",
    "    def run_experiment(self, dataloader, num_flips=10):\n",
    "        results = []\n",
    "\n",
    "        original_accuracy = self.evaluate(dataloader)\n",
    "        print(f\"Original accuracy: {original_accuracy:.2f}%\")\n",
    "\n",
    "        for flip in range(num_flips):\n",
    "            layer_name, param_index, bit_position, original_value, flipped_value = self.inject_random_bit_flip()\n",
    "            new_accuracy = self.evaluate(dataloader)\n",
    "\n",
    "            result = {\n",
    "                \"layer_name\": layer_name,\n",
    "                \"param_index\": param_index,\n",
    "                \"bit_position\": bit_position,\n",
    "                \"original_value\": original_value,\n",
    "                \"flipped_value\": flipped_value,\n",
    "                \"new_accuracy\": new_accuracy\n",
    "            }\n",
    "            results.append(result)\n",
    "            print(f\"Flip {flip+1}: Layer {layer_name}, Param {param_index}, Bit {bit_position}, \"\n",
    "                  f\"Accuracy: {new_accuracy:.2f}%\")\n",
    "\n",
    "        self.reset_weights()\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1376b795288f4756b85a343d328cdcc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8caad535b5f4fba824c0a8ec9d216d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097c91fed34f4fb3a59c42d9950e515c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57708d5522944ff49398646a1487791e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494b93d13ce44912b65dbc0ea5d33bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ce6668c5e54467a88ab4524e6db430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7d5c7a7467493a8b0915fcfc6403fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951feaeddcf648c797d08e5125165204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f432ae3bbca4be885876ca0d0a8d183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m TransformerEvaluator()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Run the experiment\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_flips\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "Cell \u001b[0;32mIn[4], line 72\u001b[0m, in \u001b[0;36mTransformerEvaluator.run_experiment\u001b[0;34m(self, dataloader, num_flips)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_experiment\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataloader, num_flips\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     70\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 72\u001b[0m     original_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m flip \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_flips):\n",
      "Cell \u001b[0;32mIn[4], line 32\u001b[0m, in \u001b[0;36mTransformerEvaluator.evaluate\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m---> 32\u001b[0m         input_ids, attention_mask, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[1;32m     34\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mlogits, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 32\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m---> 32\u001b[0m         input_ids, attention_mask, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch)\n\u001b[1;32m     33\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[1;32m     34\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mlogits, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Custom Dataset class for tokenized IMDB data\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataset_split, tokenizer, max_length=128):\n",
    "        self.data = dataset_split\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx][\"text\"]\n",
    "        label = self.data[idx][\"label\"]\n",
    "        encoded = self.tokenizer(text, truncation=True, padding='max_length',\n",
    "                                 max_length=self.max_length, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Load data and preprocess\n",
    "def get_dataloader(tokenizer, split=\"test\", batch_size=16):\n",
    "    dataset = load_dataset(\"imdb\", split=split)\n",
    "    dataset = dataset.map(lambda x: {\"label\": 1 if x[\"label\"] == \"pos\" else 0})  # Binary labels: pos=1, neg=0\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    imdb_dataset = IMDBDataset(dataset, tokenizer)\n",
    "    return DataLoader(imdb_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate the evaluator\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "dataloader = get_dataloader(tokenizer, split=\"test\", batch_size=16)\n",
    "\n",
    "evaluator = TransformerEvaluator()\n",
    "\n",
    "# Run the experiment\n",
    "results = evaluator.run_experiment(dataloader, num_flips=10)\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
