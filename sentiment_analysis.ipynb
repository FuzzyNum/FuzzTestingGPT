{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TransformerBitFlipFramework:\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", num_labels=2):\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        self.model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.original_state_dict = {k: v.clone() for k, v in self.model.state_dict().items()}  # Save original weights\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        return self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    def predict(self, text):\n",
    "        inputs = self.preprocess(text)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "    def train(self, train_dataloader, epochs=5, learning_rate=2e-5):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        self.model.train()\n",
    "\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for batch in train_dataloader:\n",
    "                input_ids, attention_mask, labels = (t.to(device) for t in batch)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "                loss = loss_fn(outputs.logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        total, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids, attention_mask, labels = (t.to(device) for t in batch)\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "                _, preds = torch.max(outputs.logits, dim=1)\n",
    "                total += labels.size(0)\n",
    "                correct += (preds == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        return accuracy\n",
    "\n",
    "    def inject_random_bit_flip(self):\n",
    "        import numpy as np\n",
    "        param_name, param_tensor = random.choice(list(self.model.named_parameters()))\n",
    "        param_data = param_tensor.detach().cpu().numpy()\n",
    "        param_flat = param_data.flatten()\n",
    "        byte_index = random.randint(0, param_flat.nbytes - 1) \n",
    "        bit_position = random.randint(0, 7)  \n",
    "        param_as_bytes = np.frombuffer(param_flat.view(np.uint8), dtype=np.uint8)\n",
    "        param_as_bytes[byte_index] ^= 1 << bit_position\n",
    "        modified_param = torch.tensor(\n",
    "            param_flat.reshape(param_data.shape), dtype=param_tensor.dtype\n",
    "        )\n",
    "        param_tensor.data = modified_param.to(param_tensor.device)\n",
    "\n",
    "        return param_name, byte_index, bit_position\n",
    "\n",
    "\n",
    "    def reset_model_weights(self):\n",
    "        # Reset weights to original state\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.model.named_parameters():\n",
    "                param.copy_(self.original_state_dict[name])\n",
    "\n",
    "    def run_experiment(self, val_dataloader, num_flips=10):\n",
    "        original_accuracy = self.evaluate(val_dataloader)\n",
    "        print(f\"Original Accuracy: {original_accuracy:.2f}%\")\n",
    "        flip_results = []\n",
    "\n",
    "        for i in range(num_flips):\n",
    "            param_name, byte_index, bit_position = self.inject_random_bit_flip()\n",
    "            new_accuracy = self.evaluate(val_dataloader)\n",
    "            flip_results.append((param_name, byte_index, bit_position, new_accuracy))\n",
    "            print(f\"Flip {i+1}: {param_name}, Byte {byte_index}, Bit {bit_position}, Accuracy: {new_accuracy:.2f}%\")\n",
    "\n",
    "            # Reset the model weights to avoid cumulative effects\n",
    "            self.reset_model_weights()\n",
    "\n",
    "        return flip_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 5.4888\n",
      "Epoch 2/3, Loss: 0.0387\n",
      "Epoch 3/3, Loss: 0.0068\n",
      "Accuracy on IMDb test set: 100.00%\n",
      "Original Accuracy: 100.00%\n",
      "Flip 1: distilbert.transformer.layer.0.output_layer_norm.weight, Byte 2328, Bit 5, Accuracy: 100.00%\n",
      "Flip 2: distilbert.embeddings.LayerNorm.weight, Byte 2775, Bit 5, Accuracy: 98.60%\n",
      "Flip 3: distilbert.transformer.layer.5.attention.q_lin.bias, Byte 684, Bit 4, Accuracy: 98.60%\n",
      "Flip 4: distilbert.transformer.layer.1.output_layer_norm.weight, Byte 1657, Bit 5, Accuracy: 98.60%\n",
      "Flip 5: distilbert.transformer.layer.0.sa_layer_norm.bias, Byte 945, Bit 0, Accuracy: 98.60%\n",
      "Bit Flip Experiment Results:\n",
      "('distilbert.transformer.layer.0.output_layer_norm.weight', 2328, 5, 100.0)\n",
      "('distilbert.embeddings.LayerNorm.weight', 2775, 5, 98.6)\n",
      "('distilbert.transformer.layer.5.attention.q_lin.bias', 684, 4, 98.6)\n",
      "('distilbert.transformer.layer.1.output_layer_norm.weight', 1657, 5, 98.6)\n",
      "('distilbert.transformer.layer.0.sa_layer_norm.bias', 945, 0, 98.6)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Define a custom dataset class for IMDb\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, tokenizer, split=\"train\", max_length=128):\n",
    "        self.dataset = load_dataset(\"imdb\", split=split)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx][\"text\"]\n",
    "        label = self.dataset[idx][\"label\"]\n",
    "        inputs = self.tokenizer(\n",
    "            text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        return inputs[\"input_ids\"].squeeze(0), inputs[\"attention_mask\"].squeeze(0), torch.tensor(label)\n",
    "\n",
    "# Initialize framework and dataset\n",
    "framework = TransformerBitFlipFramework()\n",
    "\n",
    "# Load IMDb dataset for training and evaluation\n",
    "train_dataset = IMDbDataset(framework.tokenizer, split=\"train[:5000]\")  # Use a subset for speed\n",
    "test_dataset = IMDbDataset(framework.tokenizer, split=\"test[:1000]\")    # Use a subset for evaluation\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Train the model\n",
    "framework.train(train_dataloader, epochs=3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = framework.evaluate(test_dataloader)\n",
    "print(f\"Accuracy on IMDb test set: {accuracy:.2f}%\")\n",
    "\n",
    "# Run experiment with bit flips\n",
    "results = framework.run_experiment(test_dataloader, num_flips=5)\n",
    "print(\"Bit Flip Experiment Results:\")\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
